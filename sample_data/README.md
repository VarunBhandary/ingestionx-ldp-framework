# Sample Data for Testing

This directory contains sample data files for testing the autoloader framework with Azure Data Lake Storage.

## ğŸ“ Files Included

### CSV Files
- `customers.csv` - Customer demographics (10 records)
- `products.csv` - Product catalog (10 records)

### JSON Files  
- `orders.json` - Order records with nested structure (10 records)
- `order_items.json` - Order line items (10 records)

### Parquet Files
- `*.parquet` - Same data in Parquet format for performance testing

## âš™ï¸ Configuration Files

### `test_config_adls.tsv`
Ready-to-use configuration for ADLS testing:
- Pre-configured ADLS Gen2 paths
- Different cluster sizes and schedules
- Multiple file formats
- Production-ready autoloader options

### `upload_to_adls.sh`
Script to upload sample data to your ADLS account

## ğŸš€ Quick Test Setup

### 1. Generate Parquet Files
```bash
cd sample_data
python generate_parquet.py
```

### 2. Upload Sample Data
```bash
# Edit with your storage details
nano upload_to_adls.sh

# Upload files
./upload_to_adls.sh
```

### 3. Use Test Configuration
```bash
# Copy test config
cp sample_data/test_config_adls.tsv config/ingestion_config.tsv

# Update with your storage account name
sed -i 's/oneenvadls/YOUR_STORAGE_ACCOUNT/g' config/ingestion_config.tsv
```

### 4. Deploy and Test
```bash
databricks bundle deploy --profile dev
```

## ğŸ“Š Data Characteristics

| Dataset | Format | Records | Key Features |
|---------|--------|---------|--------------|
| **Customers** | CSV | 10 | Demographics, addresses |
| **Products** | CSV | 10 | Catalog with prices |
| **Orders** | JSON | 10 | Nested structure |
| **Order Items** | JSON | 10 | Line-level detail |

### Data Relationships
- **Customers** â†’ **Orders** (1:many)
- **Orders** â†’ **Order Items** (1:many)  
- **Products** â†’ **Order Items** (1:many)

## ğŸ§ª Testing Scenarios

The sample data supports:

âœ… **Multiple File Formats** - CSV, JSON, Parquet  
âœ… **Schema Evolution** - Add columns to test evolution  
âœ… **Data Quality** - Some records with missing values  
âœ… **Incremental Loading** - Copy files incrementally  
âœ… **Nested Structures** - JSON with complex schemas

## ğŸ“‹ ADLS Structure

Organize your test data like this:

```
your-container/
â”œâ”€â”€ astellas/
â”‚   â”œâ”€â”€ csv/           # CSV files
â”‚   â”œâ”€â”€ json/          # JSON files  
â”‚   â””â”€â”€ parquet/       # Parquet files (not used in current config)
â””â”€â”€ checkpoint/        # Auto Loader checkpoints
    â”œâ”€â”€ customers/
    â”œâ”€â”€ orders/
    â”œâ”€â”€ order_items/
    â””â”€â”€ products/
```

## ğŸ¯ Expected Results

After deployment, you'll have:
- 4 DLT pipelines processing different data sources
- Data flowing into `vbdemos.adls_bronze.*` tables
- Different cluster configurations (medium, large, serverless)
- Various processing schedules (5min, 10min, 15min)

## ğŸ”§ Generate More Data

```bash
# Create additional test files
python generate_parquet.py

# This creates larger datasets for load testing
```

Happy testing! ğŸš€